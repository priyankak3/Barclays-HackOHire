{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Unsupervised Price Anomaly Detection using R\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:23:41.71851Z","iopub.status.busy":"2023-10-04T01:23:41.717748Z","iopub.status.idle":"2023-10-04T01:23:42.25013Z","shell.execute_reply":"2023-10-04T01:23:42.24951Z","shell.execute_reply.started":"2023-10-04T01:23:41.718446Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime as dt\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:23:42.27611Z","iopub.status.busy":"2023-10-04T01:23:42.275782Z","iopub.status.idle":"2023-10-04T01:23:42.40236Z","shell.execute_reply":"2023-10-04T01:23:42.401516Z","shell.execute_reply.started":"2023-10-04T01:23:42.276081Z"},"trusted":true},"outputs":[],"source":["# first dataset\n","df1 = pd.read_csv('../input/benchmark-labeled-anomaly-detection-ts/cpu4.csv')\n","df1 = df1.sort_values(by='timestamp', ascending=True)\n","df1 = df1.replace({'label': {0.0: False, 1.0: True}})\n","\n","# second dataset\n","df2 = pd.read_csv('../input/benchmark-labeled-anomaly-detection-ts/g.csv')\n","df2 = df2.sort_values(by='timestamp', ascending=True)\n","df2 = df2.replace({'label': {0.0: False, 1.0: True}})\n","\n","df1.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["## Frequency of Anomalies\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-10-04T01:23:42.405586Z","iopub.status.busy":"2023-10-04T01:23:42.405066Z","iopub.status.idle":"2023-10-04T01:23:42.424802Z","shell.execute_reply":"2023-10-04T01:23:42.423627Z","shell.execute_reply.started":"2023-10-04T01:23:42.40554Z"},"trusted":true},"outputs":[],"source":["print('Frequencies:')\n","print(df1.label.value_counts(normalize=True), '\\n')\n","print(df2.label.value_counts(normalize=True))"]},{"cell_type":"markdown","metadata":{},"source":["## Anomalies in the Dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-10-04T01:23:42.426848Z","iopub.status.busy":"2023-10-04T01:23:42.426262Z","iopub.status.idle":"2023-10-04T01:23:46.978912Z","shell.execute_reply":"2023-10-04T01:23:46.978299Z","shell.execute_reply.started":"2023-10-04T01:23:42.426797Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(12,4))\n","plt.title('Dataset 1')\n","sns.scatterplot(data=df1, x=\"timestamp\", y=\"value\", hue=\"label\")\n","plt.show()\n","\n","plt.figure(figsize=(12,4))\n","plt.title('Dataset 2')\n","sns.scatterplot(data=df2, x=\"timestamp\", y=\"value\", hue=\"label\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-10-04T01:23:46.980585Z","iopub.status.busy":"2023-10-04T01:23:46.979814Z","iopub.status.idle":"2023-10-04T01:23:47.5268Z","shell.execute_reply":"2023-10-04T01:23:47.52589Z","shell.execute_reply.started":"2023-10-04T01:23:46.980552Z"},"trusted":true},"outputs":[],"source":["df1_zoom = df1[800:1100]\n","df1_zoom_exp = df1[300:1800]\n","plt.figure(figsize=(12,4))\n","sns.scatterplot(data=df1_zoom, x=\"timestamp\", y=\"value\", hue=\"label\")\n","plt.show()\n","\n","df2_zoom = df2[1900:2200]\n","plt.figure(figsize=(12,4))\n","sns.scatterplot(data=df2_zoom, x=\"timestamp\", y=\"value\", hue=\"label\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-10-04T01:23:47.528453Z","iopub.status.busy":"2023-10-04T01:23:47.52813Z","iopub.status.idle":"2023-10-04T01:23:47.538739Z","shell.execute_reply":"2023-10-04T01:23:47.538183Z","shell.execute_reply.started":"2023-10-04T01:23:47.528412Z"},"trusted":true},"outputs":[],"source":["def plot_anomalies(df, algorithm, parameters, dumping=False, casting=None):\n","    '''Plot the Streaming Data (an Anomalies)'''\n","    Y = df.value\n","    X = df.timestamp\n","    X_pred = df.timestamp if casting is None else X.apply(casting)\n","    # predict anomalies\n","    model = algorithm(**parameters)\n","    preds = [model.detect(i, v, dumping=True) for i, v in zip(X_pred, Y)]\n","    pred, values, stds = tuple(zip(*preds))\n","    # plot the results\n","    plt.figure(figsize=(12,4))\n","    model_name = algorithm.__name__\n","    plt.title(f'Anomaly Detection - {model_name}')\n","    af  = pd.DataFrame(data={'x':X, 'value':Y, 'pred':pred})\n","    af2 = pd.DataFrame(data={'x':X, 'value':values, 'pred':pred, 'std': stds})\n","    af2['ymin'] = af2['value'] - af2['std']\n","    af2['ymax'] = af2['value'] + af2['std']\n","    size = (af.pred.astype(int)+1) * 40\n","    sns.scatterplot(data=af, x='x', y='value', hue='pred', s=size)\n","    if dumping: plt.fill_between(af2.x, af2.ymin, af2.ymax, facecolor='green', alpha=0.2)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ma\"></a>\n","## Moving Average\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-10-04T01:23:47.540479Z","iopub.status.busy":"2023-10-04T01:23:47.539734Z","iopub.status.idle":"2023-10-04T01:23:47.562461Z","shell.execute_reply":"2023-10-04T01:23:47.561619Z","shell.execute_reply.started":"2023-10-04T01:23:47.540427Z"},"trusted":true},"outputs":[],"source":["class StreamingMovingAverage:\n","    '''Moving Average algorithm'''\n","    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\n","\n","    def __init__(self, threshold=1.0) -> None:\n","        # Parameters\n","        self.max_deviation_from_expected = threshold\n","        self.min_nof_records_in_model = 3\n","        self.max_nof_records_in_model = 3 * self.min_nof_records_in_model\n","\n","    def detect(self, timestamp: int, value: float, dumping: bool=False) -> bool:\n","        '''Detect if is a Anomaly'''\n","        self._update_state(timestamp, value)\n","        expected_value = self._expected_value(timestamp)\n","        # is there enough data and is not NaN value\n","        response, curr_value, deviation = False, value, 0.0\n","        if self._enough_data() and not np.isnan(expected_value):\n","            # is the value out of the boundary? when it decrease\n","            curr_value = expected_value\n","            deviation = self._standard_deviation() * self.max_deviation_from_expected\n","            # when it is higher than expected\n","            if expected_value + deviation < value or\\\n","               expected_value - deviation > value:\n","                response = True\n","        # dumping or not\n","        if dumping: return (response, curr_value, deviation)\n","        else: return response\n","\n","    def _update_state(self, timestamp: int, value: float) -> None:\n","        '''Update the model state'''\n","        # check if it is the first time the model is run or if there is a big interval between the timestamps\n","        if not hasattr(self, 'previous_timestamp'):\n","            self._init_state(timestamp)\n","        # update the model state\n","        self.previous_timestamp = timestamp\n","        self.data_streaming.append(value)\n","        # is there a lot of data? remove one record\n","        if len(self.data_streaming) > self.max_nof_records_in_model:\n","            self.data_streaming.pop(0)\n","\n","    def _init_state(self, timestamp: int) -> None:\n","        '''Reset the parameters'''\n","        self.previous_timestamp = timestamp\n","        self.data_streaming = list()\n","\n","    def _enough_data(self) -> bool:\n","        '''Check if there is enough data'''\n","        return len(self.data_streaming) >= self.min_nof_records_in_model\n","\n","    def _expected_value(self, timestamp: int) -> float:\n","        '''Return the expected value'''\n","        data = self.data_streaming\n","        data = pd.Series(data=data, dtype=float)\n","        many = self.min_nof_records_in_model\n","        return data.rolling(many, min_periods=1).mean().iloc[-1]\n","\n","    def _standard_deviation(self) -> float:\n","        '''Return the standard deviation'''\n","        data = self.data_streaming\n","        return np.std(data, axis=0)\n","\n","    def get_state(self) -> dict:\n","        '''Get the state'''\n","        self_dict = {key: value for key, value in self.__dict__.items()}\n","        return pickle.dumps(self_dict, 4)\n","\n","    def set_state(self, state) -> None:\n","        '''Set the state'''\n","        _self = self\n","        ad = pickle.loads(state)\n","        for key, value in ad.items():\n","            setattr(_self, key, value)\n","\n","# Example\n","# ad = StreamingMovingAverage(threshold=1.5)\n","# ad.detect(timestamp=123, value=10)\n","# ad.detect(timestamp=124, value=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-10-04T01:23:47.564402Z","iopub.status.busy":"2023-10-04T01:23:47.563917Z","iopub.status.idle":"2023-10-04T01:23:48.379384Z","shell.execute_reply":"2023-10-04T01:23:48.378628Z","shell.execute_reply.started":"2023-10-04T01:23:47.56436Z"},"trusted":true},"outputs":[],"source":["parameters = {'threshold': 1.5}\n","plot_anomalies(df1_zoom, StreamingMovingAverage, parameters, dumping=True)\n","\n","parameters = {'threshold': 1.5}\n","plot_anomalies(df2_zoom, StreamingMovingAverage, parameters, dumping=True)"]},{"cell_type":"markdown","metadata":{},"source":["-----\n","<a id=\"ema\"></a>\n","\n","## Exponential Moving Average\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-10-04T01:23:48.381219Z","iopub.status.busy":"2023-10-04T01:23:48.380567Z","iopub.status.idle":"2023-10-04T01:23:48.388346Z","shell.execute_reply":"2023-10-04T01:23:48.387471Z","shell.execute_reply.started":"2023-10-04T01:23:48.381185Z"},"trusted":true},"outputs":[],"source":["class StreamingExponentialMovingAverage(StreamingMovingAverage):\n","    '''Exponential Weighted Moving Average (EWMA) algorithm'''\n","    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ewm.html\n","\n","    def __init__(self, threshold=1.0, alpha=0.3) -> None:\n","        super().__init__()\n","        # Parameters\n","        self.max_deviation_from_expected = threshold\n","        self.alpha = alpha\n","\n","    def _enough_data(self) -> bool:\n","        '''Check if there is enough data'''\n","        return len(self.data_streaming) > 0\n","\n","    def _expected_value(self, timestamp: int) -> float:\n","        '''Return the expected value'''\n","        data = self.data_streaming\n","        data = pd.Series(data=data, dtype=float)\n","        return data.ewm(alpha=self.alpha, adjust=True).mean().iloc[-1]\n","\n","# Example\n","# ad = StreamingExponentialMovingAverage(threshold=1.5, alpha=0.45)\n","# ad.detect(timestamp=123, value=10)\n","# ad.detect(timestamp=124, value=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-10-04T01:23:48.389913Z","iopub.status.busy":"2023-10-04T01:23:48.389702Z","iopub.status.idle":"2023-10-04T01:23:49.229427Z","shell.execute_reply":"2023-10-04T01:23:49.228498Z","shell.execute_reply.started":"2023-10-04T01:23:48.389887Z"},"trusted":true},"outputs":[],"source":["parameters = {'threshold': 1.5, 'alpha': 0.45}\n","plot_anomalies(df1_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)\n","\n","parameters = {'threshold': 1.5, 'alpha': 0.45}\n","plot_anomalies(df2_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)"]},{"cell_type":"markdown","metadata":{},"source":["-----\n","<a id=\"sma\"></a>\n","\n","## Seasonal Moving Average\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-10-04T01:23:49.231209Z","iopub.status.busy":"2023-10-04T01:23:49.230899Z","iopub.status.idle":"2023-10-04T01:23:49.249785Z","shell.execute_reply":"2023-10-04T01:23:49.248599Z","shell.execute_reply.started":"2023-10-04T01:23:49.231167Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime as dt\n","\n","from typing import Union\n","\n","MINUTES_HOUR = 60 # minutes per hour\n","MINUTES_DAY = 24 * MINUTES_HOUR # minutes per day\n","NUMBER_DAYS_WEEK = 7 # number days a week\n","\n","\n","class AnomalyDetectionSeasonalBucket(StreamingMovingAverage):\n","\n","    def __init__(self, min_buckets: int, window: int=4, min_value: int=10, threshold: float=2) -> None:\n","        '''Min number of messages is 1 per minute'''\n","        super().__init__()\n","        # parameters\n","        self.min_buckets = min_buckets\n","        self._num_bucket = int(np.floor(MINUTES_DAY / min_buckets))\n","        self.window_size = window\n","        self.min_value = min_value\n","        self.max_deviation_from_expected = threshold\n","\n","    def _get_cday_cbucket(self, timestamp: dt.datetime) -> tuple:\n","        '''Get cday, cbucket values from timestamp'''\n","        # compute bucket indexes\n","        cday, chour, cminute = timestamp.weekday(), timestamp.hour, timestamp.minute\n","        cbucket = int(np.floor((chour * MINUTES_HOUR + cminute) / self.min_buckets))\n","        return cday, cbucket\n","    \n","    def _create_copy_data(self, timestamp: dt.datetime) -> np.array:\n","        '''Get data based on timestamp'''\n","        # compute bucket indexes, get data\n","        cday, cbucket = self._get_cday_cbucket(timestamp = timestamp)\n","        data = np.copy(self._buckets[cday, cbucket])\n","        data[data == 0.0] = np.nan\n","        return data\n","    \n","    def _update_state(self, timestamp: dt.datetime, value: float) -> None:\n","        '''Update the model state'''\n","        # check if it is the first run\n","        if not hasattr(self, 'previous_timestamp'):\n","            self._init_state(timestamp)\n","        # compute bucket indexes\n","        cday, cbucket = self._get_cday_cbucket(timestamp = timestamp)\n","        # shift values, empty days\n","        pass_days = int(np.ceil((timestamp - self._buckets_timestamp[cday, cbucket]).days / NUMBER_DAYS_WEEK))\n","        self._buckets[cday, cbucket] = np.roll(self._buckets[cday, cbucket], pass_days)\n","        self._buckets[cday, cbucket, 0:pass_days] = 0\n","        # update the model state\n","        self.previous_timestamp = timestamp\n","        self._buckets_timestamp[cday, cbucket] = timestamp\n","        self._buckets[cday, cbucket, 0] = self._buckets[cday, cbucket, 0] + value\n","\n","    def _init_state(self, timestamp: dt.datetime) -> None:\n","        '''Reset the parameters'''\n","        self.previous_timestamp = timestamp\n","        self._buckets = np.zeros((NUMBER_DAYS_WEEK, self._num_bucket, self.window_size))\n","        self._buckets_timestamp = np.full((NUMBER_DAYS_WEEK, self._num_bucket), timestamp, dtype=dt.datetime)\n","\n","    def _expected_value(self, timestamp: dt.datetime) -> float:\n","        '''Return the expected value'''\n","        data = self._create_copy_data(timestamp = timestamp)\n","        return np.nanmean(data)\n","\n","    def _enough_data(self) -> bool:\n","        '''Check if there is enough data'''\n","        data = self._create_copy_data(timestamp = self.previous_timestamp)\n","        records = self.window_size - np.sum(np.isnan(data))\n","        return records >= min(self.min_nof_records_in_model, self.window_size)\n","\n","    def _standard_deviation(self) -> float:\n","        '''Return the standard deviation'''\n","        # compute bucket indexes, get data\n","        data = self._create_copy_data(timestamp = self.previous_timestamp)\n","        return np.nanstd(data)\n","    \n","# Example\n","# ad = AnomalyDetectionSeasonalBucket(min_buckets=60, window=5)\n","# ad.detect(timestamp=dt.datetime(2023, 1, 23, 23, 40), value=10)\n","# ad.detect(timestamp=dt.datetime(2023, 1, 30, 23, 40), value=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:23:49.253824Z","iopub.status.busy":"2023-10-04T01:23:49.253136Z","iopub.status.idle":"2023-10-04T01:23:49.886023Z","shell.execute_reply":"2023-10-04T01:23:49.885184Z","shell.execute_reply.started":"2023-10-04T01:23:49.253773Z"},"trusted":true},"outputs":[],"source":["parameters = {'min_buckets': 30, 'window': 2}\n","plot_anomalies(df1_zoom, AnomalyDetectionSeasonalBucket, parameters, dumping=True, casting=dt.datetime.fromtimestamp)\n","\n","parameters = {'min_buckets': 30, 'window': 2}\n","plot_anomalies(df2_zoom, AnomalyDetectionSeasonalBucket, parameters, dumping=True, casting=dt.datetime.fromtimestamp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:23:49.888037Z","iopub.status.busy":"2023-10-04T01:23:49.887583Z","iopub.status.idle":"2023-10-04T01:23:50.593225Z","shell.execute_reply":"2023-10-04T01:23:50.592316Z","shell.execute_reply.started":"2023-10-04T01:23:49.88799Z"},"trusted":true},"outputs":[],"source":["parameters = {'min_buckets': 30, 'window': 2}\n","plot_anomalies(df1.iloc[0:2050], AnomalyDetectionSeasonalBucket, parameters, dumping=True, casting=dt.datetime.fromtimestamp)"]},{"cell_type":"markdown","metadata":{},"source":["-----\n","<a id=\"arima\"></a>\n","\n","## Auto Regressive Integrated Moving Average - ARIMA\n","\n","**ARIMA** is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration. ARIMA is capable of predict values according to its previous observations. Further, we simple check if the new record is far from the expected value. The expected value range is computed using the formula `ARIMA + standard deviation * threshold`; if it is out of the expected value range, we report as an anomaly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-10-04T01:23:50.595057Z","iopub.status.busy":"2023-10-04T01:23:50.594675Z","iopub.status.idle":"2023-10-04T01:24:00.311172Z","shell.execute_reply":"2023-10-04T01:24:00.309798Z","shell.execute_reply.started":"2023-10-04T01:23:50.595005Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install statsmodels==0.13.1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:24:00.314498Z","iopub.status.busy":"2023-10-04T01:24:00.313903Z","iopub.status.idle":"2023-10-04T01:24:00.638689Z","shell.execute_reply":"2023-10-04T01:24:00.637881Z","shell.execute_reply.started":"2023-10-04T01:24:00.31445Z"},"trusted":true},"outputs":[],"source":["import warnings\n","from statsmodels.tsa.arima.model import ARIMA\n","\n","warnings.filterwarnings('ignore')\n","\n","class StreamingARIMA(StreamingMovingAverage):\n","    '''ARIMA algorithm'''\n","\n","    def __init__(self, threshold=1.0, order:tuple=(2, 1, 2)) -> None:\n","        # Parameters\n","        self.order = order\n","        self.model = None\n","        self.res = None\n","        self.max_deviation_from_expected = threshold\n","        self.min_nof_records_in_model = 10\n","        self.max_nof_records_in_model = 3 * self.min_nof_records_in_model\n","\n","    def _expected_value(self, timestamp: int) -> float:\n","        '''Return the expected value'''\n","        if self._enough_data():\n","            data = self.data_streaming\n","            self.model = ARIMA(data, order=self.order)\n","            self.res = self.model.fit()\n","            output = self.res.forecast()\n","            return output[0]\n","        return np.nan\n","\n","    def _standard_deviation(self) -> float:\n","        '''Return the standard deviation'''\n","        much = self.min_nof_records_in_model\n","        data = self.data_streaming\n","        if len(data) > much:\n","            data = data[-much:]\n","        return np.std(data, axis=0)\n","\n","    def summary(self) -> None:\n","        '''Print the ARIMA summary'''\n","        if pd.notnull(self.res):\n","            print(self.res.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-10-04T01:24:00.639993Z","iopub.status.busy":"2023-10-04T01:24:00.639787Z","iopub.status.idle":"2023-10-04T01:24:24.516866Z","shell.execute_reply":"2023-10-04T01:24:24.515951Z","shell.execute_reply.started":"2023-10-04T01:24:00.639968Z"},"trusted":true},"outputs":[],"source":["from math import sqrt\n","from sklearn.metrics import mean_squared_error\n","\n","# walk-forward validation\n","X = df1_zoom.iloc[0:50].value.tolist()\n","tests, predictions = list(), list()\n","model = StreamingARIMA()\n","\n","for t in range(len(X)):\n","    obs = X[t]\n","    model.detect(t, obs)\n","    yhat = model._expected_value(0)\n","    if pd.notna(yhat):\n","        predictions.append(yhat)\n","        tests.append(obs)\n","# evaluate forecasts\n","rmse = sqrt(mean_squared_error(tests, predictions))\n","\n","print('Test RMSE: %.3f' % rmse)\n","# plot forecasts against actual outcomes\n","plt.plot(tests, label='data')\n","plt.plot(predictions, color='gray', label='ARIMA')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:24:24.518927Z","iopub.status.busy":"2023-10-04T01:24:24.518105Z","iopub.status.idle":"2023-10-04T01:24:24.543061Z","shell.execute_reply":"2023-10-04T01:24:24.542162Z","shell.execute_reply.started":"2023-10-04T01:24:24.51889Z"},"trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T01:24:24.545259Z","iopub.status.busy":"2023-10-04T01:24:24.544405Z"},"trusted":true},"outputs":[],"source":["%%time\n","parameters = {'threshold': 1.5, 'order': (2, 1, 2)}\n","plot_anomalies(df1_zoom, StreamingARIMA, parameters, dumping=True)\n","\n","parameters = {'threshold': 1.5, 'order': (2, 1, 2)}\n","plot_anomalies(df2_zoom, StreamingARIMA, parameters, dumping=True)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"conclusion\"></a>\n","\n","---\n","# Conclusion\n","\n","(1) We can see that **Exponential Moving Average** smooths the prediction better than the simple **Moving Average**. For the first dataset, the algorithm was able to detect the exact point of the anomaly and quickly adjust your prediction behaviour, avoiding multiples \"sequential anomaly alerts\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"source":["parameters = {'threshold': 1.5}\n","plot_anomalies(df1_zoom, StreamingMovingAverage, parameters, dumping=True)\n","\n","parameters = {'threshold': 1.5, 'alpha': 0.45}\n","plot_anomalies(df1_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)"]},{"cell_type":"markdown","metadata":{},"source":["(2) Again, we can see that **Exponential Moving Average** smooths the prediction better than the simple **Moving Average**. However, the second dataset was more difficult than the first one, due to the high variation in the data. Anyway, the algorithm was able to detect the anomaly point, also produce a few false negative.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"source":["parameters = {'threshold': 1.5}\n","plot_anomalies(df2_zoom, StreamingMovingAverage, parameters, dumping=True)\n","\n","parameters = {'threshold': 1.5, 'alpha': 0.45}\n","plot_anomalies(df2_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)"]},{"cell_type":"markdown","metadata":{},"source":["(3) the **ARIMA** seems to be promising. However it still underperforming compared to **Exponential Moving Average**."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["parameters = {'threshold': 1.5, 'alpha': 0.45}\n","plot_anomalies(df1_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)\n","\n","parameters = {'threshold': 1.5, 'order': (4, 1, 2)}\n","plot_anomalies(df1_zoom, StreamingARIMA, parameters, dumping=True)"]},{"cell_type":"markdown","metadata":{},"source":["(4) **Seasonal Moving Average** is a time-based algorithm that relies on previous days to make predictions. It did not fit for these two sample use cases because it is small amount of non-seasonal data.\n","\n","(5) In summary, it is important to use the appropriate algorithm for the type of data being analyzed. In the case of non-seasonal data, algorithms such as the **Exponential Moving Average** or the **Holt-Winters** algorithm may be more suitable for anomaly detection. These algorithms are able to adapt to changes in the data and do not rely on seasonal patterns for their analysis. Related to seasonal data, you can explore **Seasonal Moving Average** or **Seasonal Exponential Moving Average**."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1251064,"sourceId":2086563,"sourceType":"datasetVersion"}],"dockerImageVersionId":30157,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
